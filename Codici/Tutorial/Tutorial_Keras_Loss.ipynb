{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Custom Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and model\n",
    "import sys\n",
    "sys.path.append('/home/silvia/Documents/GitHub/primoprogetto/Codici/')  # PER importare cose che non sono nella stessa directory\n",
    "from Classe_sismogramma_v3 import ClasseDataset\n",
    "import keras \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "pat_tent = '/home/silvia/Documents/GitHub/primoprogetto/Codici/Tentativi/'\n",
    "tentativo = 52\n",
    "model = keras.models.load_model(pat_tent+str(tentativo)+'/Tentativo_'+str(tentativo)+'.hdf5')\n",
    "\n",
    "hdf5pol = '/home/silvia/Desktop/Data/Instance_Data/Tre_4s/data_Velocimeter_4s_Normalizzate_New1-1.hdf5'\n",
    "csvpol = '/home/silvia/Desktop/Data/Instance_Data/Tre_4s/metadata_Velocimeter_4s_Normalizzate_New1-1.csv'\n",
    "\n",
    "hdf5und = \"/home/silvia/Desktop/Data/Instance_Data/Undecidable/Instance_undecidable_data_tot_no0.hdf5\"\n",
    "csvund = \"/home/silvia/Desktop/Data/Instance_Data/Undecidable/Instance_undecidable_metadata_tot_no0.csv\"\n",
    "\n",
    "Dpol, Dund = (ClasseDataset(),ClasseDataset())\n",
    "Dpol.leggi_custom_dataset(hdf5pol,csvpol)\n",
    "Dund.leggi_custom_dataset(hdf5und,csvund)\n",
    "semi_amp = 80\n",
    "lung = len(Dund.sismogramma[0])\n",
    "\n",
    "N_pol = len(Dpol.sismogramma)\n",
    "xtest_pol = np.zeros((N_pol, semi_amp * 2))\n",
    "for k in range(N_pol):\n",
    "    xtest_pol[k] = Dpol.sismogramma[k][lung // 2 - semi_amp : lung // 2 + semi_amp]\n",
    "y_test_true_pol = np.array([Dpol.metadata[\"trace_polarity\"][_] == \"positive\" for _ in range(N_pol)])\n",
    "y_test_true_pol = y_test_true_pol + 0\n",
    "\n",
    "N_und = len(Dund.sismogramma)\n",
    "xtest_und = np.zeros((N_und, semi_amp * 2))\n",
    "for k in range(N_und):\n",
    "    xtest_und[k] = Dund.sismogramma[k][lung // 2 - semi_amp : lung // 2 + semi_amp]\n",
    "y_test_true_und = np.array([0.5 for _ in range(N_und)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTO A MONITOR di EarlyStopping!!!!!!!!!!! (voglio monitorare solo la cosa di prima???)\n",
    "from keras.utils.metrics_utils import binary_matches \n",
    "from keras import backend\n",
    "import keras\n",
    "\n",
    "def calc_CE(y_true,y_pred):\n",
    "    \"\"\"Compute mean crossentropy on data where y_true == 0 or y_true == 1\"\"\" # (Verified, it works)\n",
    "    # CE_bool shows where terms that contribute to crossentropy are \n",
    "    CE_bool = tf.cast(tf.equal(y_true, 1), backend.floatx()) + tf.cast(tf.equal(y_true, 0), backend.floatx())\n",
    "    CE = tf.keras.losses.binary_crossentropy(CE_bool*tf.cast(y_true, backend.floatx()), CE_bool*tf.cast(y_pred, backend.floatx())) * len(CE_bool)/tf.reduce_sum(CE_bool)\n",
    "    return CE\n",
    "\n",
    "\n",
    "def calc_FLT1(y_true,y_pred):\n",
    "    \"\"\"Compute MSE on data where y_true == 0.5\"\"\" # (Verified, it works)\n",
    "    # FLT_bool shows where terms that contribute to FLT are  \n",
    "    FLT_bool = tf.cast(tf.equal(y_true, 0.5), backend.floatx())\n",
    "    FLT = tf.keras.losses.mean_squared_error(FLT_bool*tf.cast(y_true, backend.floatx()), FLT_bool*tf.cast(y_pred, backend.floatx())) * len(FLT_bool)/tf.reduce_sum(FLT_bool)\n",
    "    return FLT\n",
    "\n",
    "def calc_FLT2(y_true,y_pred):\n",
    "    \"\"\"Compute MSE on data where y_true == 0.5\"\"\" # (Verified, it works)\n",
    "    # FLT_bool shows where terms that contribute to FLT are  \n",
    "    FLT_bool = tf.cast(tf.equal(y_true, 0.5), backend.floatx())\n",
    "    #FLT = tf.maximum(FLT_bool*tf.cast(y_true, backend.floatx()), FLT_bool*tf.cast(y_pred, backend.floatx())) * len(FLT_bool)/tf.reduce_sum(FLT_bool)\n",
    "    return 0 #FLT\n",
    "\n",
    "def my_loss(lamb):\n",
    "    \"\"\"\n",
    "    This custom loss for i-th sample is computed as follows:\n",
    "    if yti == 0 or yti == 1\n",
    "        loss = binarycrossentropy(yti, ypi)\n",
    "    if yti == 0.5:\n",
    "        loss = lamb * (yti - ypi)**2\n",
    "\n",
    "    I chose to represent waveforms with no definite polarity (or no events or other) labeled with yt=0.5.\n",
    "    I instruct the network to provide a response of 0.5 for out-of-distribution waveforms\n",
    "    \"\"\"\n",
    "    def calc_loss(y_true,y_pred):\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        \n",
    "        # CE CROSS ENTROPY term (verified it works)\n",
    "        # FLT \"ENFORCING LOW CONFIDENCE\" term. On out of distribution data\n",
    "        # Verified, it works\n",
    "        return (calc_CE(y_true,y_pred) + lamb * calc_FLT1(y_true,y_pred)) \n",
    "    return calc_loss\n",
    "\n",
    "\n",
    "def calc_lamb(model, y_true, xtrain):\n",
    "    \"\"\"\n",
    "    Calculating the lamb parameter \n",
    "    \"\"\"\n",
    "    y_pred = model.predict(xtrain)\n",
    "\n",
    "    return calc_CE(y_true, y_pred) / calc_FLT1(y_true, y_pred)\n",
    "\n",
    "def my_acc(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy only on definite polarity waveforms, exclude from the count other!\n",
    "    binary_matches return a tf.tensor, whoose value in i-th pos is 1 \n",
    "    when (yp_i > 0.5 and yt_i==1) or ( yp_i < 0.5 and yt_i==0), \n",
    "    else 0 (0.5 is the threshold, can be changed)\n",
    "    \"\"\"\n",
    "    # return numb of matches / (numb yt==0 + numb yt==1)\n",
    "    return tf.math.reduce_sum(binary_matches(y_true, y_pred)) / tf.math.reduce_sum (tf.cast(tf.equal(y_true, 1), backend.floatx()) + tf.cast(tf.equal(y_true, 0), backend.floatx()))\n",
    "\n",
    "\n",
    "class Cambia_Loss_Epoca(keras.callbacks.Callback):\n",
    "    \"The model is an attribute of tf.keras.callbacks.Callback, so you can access it directly with self.model\"\n",
    "    def __init__(self, xtrain, ytrain_true):\n",
    "        self.validation_data = None\n",
    "        self.model = None\n",
    "        # Whether this Callback should only run on the chief worker in a\n",
    "        # Multi-Worker setting.\n",
    "        # TODO(omalleyt): Make this attr public once solution is stable.\n",
    "        self._chief_worker_only = None\n",
    "        self._supports_tf_logs = False\n",
    "        self.xtrain = xtrain\n",
    "        self.y_true = ytrain_true\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Compute mean loss on definite polarity, separated from loss on out-of-distribution and set the lambda term in the way that their means match??\"\"\"\n",
    "        y_pred = self.model.predict(self.xtrain)\n",
    "        \n",
    "        lamb = calc_lamb(self.model, self.y_true, self.xtrain)\n",
    "        # if FLT > 2*CE or CE > 2*FLT:\n",
    "        if lamb > 2 or lamb < 0.5: \n",
    "            self.model.compile(\n",
    "                                # optimizer=optimizers.SGD(momentum=momento),  # TODO CAMBIA\n",
    "                                loss=my_loss(lamb),\n",
    "                                metrics=[my_acc]\n",
    "                            )\n",
    "            print(\"Ho cambiato, lamb=\", lamb)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# model.compile(\n",
    "#     # optimizer=optimizers.SGD(momentum=momento),  # TODO CAMBIA\n",
    "#     loss=my_loss(calc_mean_loss_for_allineare_mean(8)),\n",
    "#     metrics=[my_acc]\n",
    "# )\n",
    "\n",
    "\n",
    "# storia = model.fit (bla\n",
    "#                     bla\n",
    "#                     bla\n",
    "#                     callbacks=[EarlyStopping(patience=pazienza,  restore_best_weights=True, monitor=my_starting_loss),\n",
    "#                                Cambia_Loss_Epoca()]\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4901161193847656e-08"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify Cross entropy and my method are same \n",
    "from keras import backend\n",
    "import tensorflow as tf\n",
    "backend.floatx()\n",
    "ytrue = [0 for i in range(11)] + [0.5 for i in range(11)]  + [1 for i in range(11)] \n",
    "ypred = [i/33.0 for i in range(33)]\n",
    "CE_bool = tf.cast(tf.equal(ytrue, 1), backend.floatx()) + tf.cast(tf.equal(ytrue, 0), backend.floatx())\n",
    "\n",
    "# TERMINE DI CROSS ENTROPY VERO\n",
    "a1 = tf.keras.losses.binary_crossentropy(ytrue[0:11] + ytrue[22:],ypred[0:11] + ypred[22:])\n",
    "\n",
    "# TERMINE DI CROSS ENTROPY \"Mio\"\n",
    "#a2 = tf.keras.losses.binary_crossentropy(CE_bool*tf.cast(ytrue, backend.floatx()), CE_bool*tf.cast(ypred, backend.floatx())) * len(CE_bool)/tf.reduce_sum(CE_bool)\n",
    "a2 = calc_CE(ytrue, ypred)\n",
    "#lossi = my_loss(0.0)\n",
    "#a2 = lossi(ytrue, ypred)\n",
    "float(a1-a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.009412304, shape=(), dtype=float32)\n",
      "tf.Tensor(0.009412304, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# verify MSE and my method are same  \n",
    "from keras import backend\n",
    "import tensorflow as tf\n",
    "backend.floatx()\n",
    "ytrue = [0 for i in range(11)] + [0.5 for i in range(11)]  + [1 for i in range(11)] \n",
    "ypred = [i/33.0 for i in range(33)]\n",
    "FLT_bool = tf.cast(tf.equal(ytrue, 0.5), backend.floatx())\n",
    "\n",
    "# TERMINE DI MSE VERO\n",
    "a1 = tf.keras.losses.mean_squared_error(ytrue[11:22] ,ypred[11:22])\n",
    "\n",
    "# TERMINE DI MSE \"Mio\"\n",
    "#a2 = tf.keras.losses.mean_squared_error(FLT_bool*tf.cast(ytrue, backend.floatx()), FLT_bool*tf.cast(ypred, backend.floatx())) * len(FLT_bool)/tf.reduce_sum(FLT_bool)\n",
    "#a2 = calc_FLT(ytrue, ypred)\n",
    "lossi = my_loss(1.0)\n",
    "a2 = lossi(ytrue, ypred)\n",
    "print(a1)\n",
    "print(a2)\n",
    "print(a1-a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verufy model.loss is binary_crossentropy\n",
    "tf.keras.losses.binary_crossentropy is model.loss # output is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 30  6  8]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Definisci due tensori con la stessa dimensione\n",
    "tensor1 = tf.constant([1, 30, 5, 7])\n",
    "tensor2 = tf.constant([2, 4, 6, 8])\n",
    "\n",
    "# Calcola il massimo elemento per elemento\n",
    "max_tensor = tf.maximum(tensor1, tensor2)\n",
    "\n",
    "# Stampa il risultato\n",
    "print(max_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verify 0-entries in calc_CE does not affect gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 10:42:32.650581: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-16 10:42:34.463373: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 10:42:35.057609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5225 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:9e:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras import backend\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "import numpy as np\n",
    "def calc_CE(y_true,y_pred):\n",
    "    \"\"\"Compute mean crossentropy on data where y_true == 0 or y_true == 1\"\"\" # (Verified, it works)\n",
    "    # CE_bool shows where terms that contribute to crossentropy are \n",
    "    CE_bool = tf.cast(tf.equal(y_true, 1), backend.floatx()) + tf.cast(tf.equal(y_true, 0), backend.floatx())\n",
    "    CE = tf.keras.losses.binary_crossentropy(CE_bool*tf.cast(y_true, backend.floatx()), CE_bool*tf.cast(y_pred, backend.floatx())) * len(CE_bool)/tf.reduce_sum(CE_bool)\n",
    "    #CE = tf.keras.losses.binary_crossentropy(tf.cast(y_true, backend.floatx()), tf.cast(y_pred, backend.floatx()))\n",
    "    return CE\n",
    "\n",
    "def create_identity(x):\n",
    "    vinput_shape = (None, 20, 1)\n",
    "    identity = Conv1D(filters=1, kernel_size=1, activation=None, input_shape=vinput_shape[1:])\n",
    "    x_0 = x\n",
    "    x_0 = np.expand_dims(x_0, axis=0)\n",
    "    x_0 = np.expand_dims(x_0, axis=-1)\n",
    "    aia = identity(x_0)                \n",
    "    identity.set_weights([np.array([[[1.0]]]), np.array([0.0])])\n",
    "    return identity\n",
    "\n",
    "\n",
    "ytrue = [0 for i in range(1)] + [0.5 for i in range(1)]  + [1 for i in range(1)] \n",
    "x = np.array([[i/20.0 +j for i in range(20)]for j in range(3)])\n",
    "\n",
    "\n",
    "\n",
    "ID = create_identity(x[0])\n",
    "fl = Flatten()\n",
    "layD = Dense(1, activation=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad  [[ 0.01531619]\n",
      " [-0.00386189]\n",
      " [-0.00439651]\n",
      " [-0.02264871]\n",
      " [ 0.01406922]\n",
      " [-0.00192071]\n",
      " [ 0.01530548]\n",
      " [ 0.00281429]\n",
      " [-0.01425789]\n",
      " [-0.00090934]\n",
      " [-0.02470191]\n",
      " [ 0.00739589]\n",
      " [-0.01277691]\n",
      " [-0.00793162]\n",
      " [-0.02568799]\n",
      " [ 0.00436738]\n",
      " [-0.00963266]\n",
      " [-0.00122552]\n",
      " [ 0.02891251]\n",
      " [-0.0202882 ]]\n",
      "yrue  1\n"
     ]
    }
   ],
   "source": [
    "# verifica se faccio binary crossentropy va bene lo stesso. poi verifica manualemnte i gradienti??\n",
    "with tf.GradientTape() as tape:\n",
    "    indx = 2\n",
    "    x_0 = x[indx]\n",
    "    x_0 = np.expand_dims(x_0, axis=0)\n",
    "    x_0 = np.expand_dims(x_0, axis=-1)\n",
    "    act = ID(x_0)\n",
    "    act2 = fl(act)\n",
    "    act3 = layD(act2)\n",
    "    Loss = calc_CE([ytrue[indx]],[act3])\n",
    "    grad = tape.gradient(Loss,act)\n",
    "grad = np.array(grad)\n",
    "grad = grad[0]\n",
    "print(\"grad \",grad)\n",
    "print(\"yrue \",ytrue[indx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
